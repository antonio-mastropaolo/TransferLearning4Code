{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PeYZFcZfqDm"
      },
      "outputs": [],
      "source": [
        "#Installing libraries\n",
        "!pip install transformers==3.0.2\n",
        "!pip install nlp==0.4.0\n",
        "!pip install pyarrow==0.16.0\n",
        "!pip install sentencepiece==0.1.96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPJrfdXsatFu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nlp\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EgLgEVkfra5"
      },
      "outputs": [],
      "source": [
        "#Befere running evaluation we have to convert tensorflow checkpoint into pytorch model.\n",
        "#See here: https://github.com/huggingface/transformers/blob/master/src/transformers/convert_t5_original_tf_checkpoint_to_pytorch.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XiwYcFffszU"
      },
      "outputs": [],
      "source": [
        "#Import the tokenizer and the config file from drive\n",
        "#The config file can be download from this link: https://s3.amazonaws.com/models.huggingface.co/bert/t5-small-config.json\n",
        "\n",
        "spm_path = '/content/drive/MyDrive/T5-Extension/Tokenizer/dl4se.model'\n",
        "config_file = '/content/drive/MyDrive/T5-Extension/Trained-Models/with-pretraining-new/MT-Task-Balanced/Pytorch-Model/config.json'\n",
        "config = T5Config.from_json_file(config_file)\n",
        "tokenizer = T5Tokenizer.from_pretrained(spm_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib4MdfDSSZRJ"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/T5-Extension/Miscellaneous/bfp_dataset_script.py /content/bfp_dataset_script.py\n",
        "#!cp /content/drive/MyDrive/T5-Extension/Miscellaneous/bfp_dataset_script.py /content/assert_dataset_script.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqYdPg2Rfuhw"
      },
      "outputs": [],
      "source": [
        "# Change the prefix when want to evaluate different tasks\n",
        "# (1) generate small patch\n",
        "# (2) generate medium patch\n",
        "# (3) generate abt assert\n",
        "# (4) generate raw assert\n",
        "\n",
        "# If you're evaluating abt/raw assert generative tasks, change example['method'].lower() for the input_text and  example['assertion'].lower() for the target_text\n",
        "\n",
        "def add_eos_to_examples(example):\n",
        "    example['input_text'] = 'generate small patch: %s </s>' % example['buggy']#.lower()\n",
        "    example['target_text'] = '%s </s>' % example['fixed']#.lower()\n",
        "    return example\n",
        "\n",
        "\n",
        "def convert_to_features(example_batch):\n",
        "    input_encodings = tokenizer.batch_encode_plus(example_batch['input_text'], pad_to_max_length=True, max_length=512)\n",
        "    target_encodings = tokenizer.batch_encode_plus(example_batch['target_text'], pad_to_max_length=True, max_length=512)\n",
        "\n",
        "    encodings = {\n",
        "        'input_ids': input_encodings['input_ids'], \n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'target_ids': target_encodings['input_ids'],\n",
        "        'target_attention_mask': target_encodings['attention_mask']\n",
        "    }\n",
        "\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_7N1u-NtHMA"
      },
      "outputs": [],
      "source": [
        "#!cp '/content/drive/MyDrive/T5-Extension/Datasets/tsv/fine-tuning/BFmedium/test.tsv' test.tsv\n",
        "#!cp '/content/drive/MyDrive/T5-Extension/Datasets/tsv/fine-tuning/AGraw/test.tsv' test.tsv\n",
        "#!cp '/content/drive/MyDrive/T5-Extension/datasets/tsv/fine-tuning/CS/test.tsv' test.tsv\n",
        "#!cp '/content/drive/MyDrive/T5-Extension/datasets/tsv/fine-tuning/MG/test.tsv' test.tsv\n",
        "#!cp '/content/drive/MyDrive/T5-Extension/Datasets/tsv/fine-tuning/AGabs/test.tsv' test.tsv\n",
        "#!cp '/content/drive/MyDrive/T5-Extension/Datasets/tsv/fine-tuning/BFsmall/test.tsv' test.tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q360jPaLfxMx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Here we have to change the script for loading the dataset\n",
        "# Pick the script according to the task and load it on this colab instance\n",
        "# Make sure to load the test set as well; otherwise, it doesn't work.\n",
        "\n",
        "valid_dataset = nlp.load_dataset('/content/bfp_dataset_script.py', split=nlp.Split.TEST)\n",
        "\n",
        "\n",
        "# map add_eos_to_examples function to the dataset example wise \n",
        "valid_dataset = valid_dataset.map(add_eos_to_examples, load_from_cache_file=False)\n",
        "\n",
        "# map convert_to_features batch wise\n",
        "valid_dataset = valid_dataset.map(convert_to_features, batched=True, load_from_cache_file=False)\n",
        "\n",
        "\n",
        "columns = ['input_ids', 'target_ids', 'attention_mask','target_attention_mask']\n",
        "valid_dataset.set_format(type='torch', columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Vgfly4f0JB"
      },
      "outputs": [],
      "source": [
        "#The BATCH_SIZE must be set according to the available VRAM.\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9incoTgAwZ7_"
      },
      "outputs": [],
      "source": [
        "#Let's import the ground truth from the test dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('test.tsv',header=None,sep='\\t')\n",
        "\n",
        "references=[]\n",
        "\n",
        "for item in df[1]:\n",
        "  references.append(item)\n",
        "\n",
        "inputs=[]\n",
        "\n",
        "for item in df[0]:\n",
        "  inputs.append(item)\n",
        "\n",
        "inputs[0], references[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LPA2aw6egEgr"
      },
      "outputs": [],
      "source": [
        "#Set CUDA device to leverage GPU computation\n",
        "CUDA = torch.device(\"cuda\")\n",
        "\n",
        "finetuned_model_path = '/content/drive/MyDrive/T5-Extension/Trained-Models/with-pretraining-new/MT-Task-Balanced/Pytorch-Model/pytorch_model.bin'\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "        finetuned_model_path,\n",
        "        config=config\n",
        "        ).to(CUDA)\n",
        "        \n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KQ94kR2gJFL"
      },
      "outputs": [],
      "source": [
        "# Change the max_length in model.generate according to specific tasks\n",
        "# For bfp_small and bfp_medium we set respectively 128 and 256.\n",
        "# For both abt assert and raw assert tasks, we used 512 as max length\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "predictions = []\n",
        "\n",
        "BEAM_SIZE = 25\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for batch in tqdm(dataloader):\n",
        "\n",
        "      outs = model.generate(\n",
        "                          input_ids=batch['input_ids'].to(CUDA),\n",
        "                          attention_mask=batch['attention_mask'].to(CUDA),\n",
        "                          num_beams=BEAM_SIZE, \n",
        "                          max_length=128,\n",
        "                          num_return_sequences=BEAM_SIZE, \n",
        "                          early_stopping=True\n",
        "                          )\n",
        "    \n",
        "\n",
        "    \n",
        "      outs = [tokenizer.decode(ids, skip_special_tokens=True)  for ids in outs]\n",
        "      predictions.extend(outs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLKmlAJJO1M9"
      },
      "outputs": [],
      "source": [
        "pred_refined = []\n",
        "for pred in predictions:\n",
        "    if len(pred)>=2:\n",
        "      if pred[0]=='\"':\n",
        "          pred = pred[1:]\n",
        "      if pred[-1]=='\"':\n",
        "          pred = pred[:-1]\n",
        "    pred_refined.append(pred)\n",
        "    \n",
        "len(pred_refined),len(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhskxiWlgA2W"
      },
      "outputs": [],
      "source": [
        "counter_pred = 0\n",
        "\n",
        "mispred_list = []\n",
        "\n",
        "sanity_check_list = []\n",
        "\n",
        "idx = 0\n",
        "\n",
        "len_prediction=(len(pred_refined))\n",
        "\n",
        "\n",
        "for i in range(0, len_prediction, BEAM_SIZE):\n",
        "\n",
        "    items_to_analyze = pred_refined[i:i+BEAM_SIZE]\n",
        "    target_item = ''.join(references[idx].split(' '))\n",
        "    \n",
        "    flag_perfect = False\n",
        "\n",
        "    \n",
        "    fpred=open('/content/drive/MyDrive/T5-Extension/Results/Predictions/Multi-Task/Balanced/BF-Task/Small/prediction_@{}.txt'.format(BEAM_SIZE),'a+')\n",
        "    fpred.write('************************************\\n')\n",
        "    fpred.write('[+] input: {}\\n'.format(inputs[idx]))\n",
        "        \n",
        "\n",
        "    for pred in items_to_analyze:\n",
        "        \n",
        "        pred_ref = ''.join(pred.split(' '))\n",
        "        \n",
        "        fpred.write('[*] target: {}\\n'.format(references[idx]))\n",
        "        fpred.write('[-] pred:  {}\\n\\n'.format(pred))\n",
        "\n",
        "        if pred_ref == target_item and not flag_perfect:\n",
        "            counter_pred+=1\n",
        "            sanity_check_list.append(pred)\n",
        "\n",
        "            with open('/content/drive/MyDrive/T5-Extension/Results/Predictions/Multi-Task/Balanced/BF-Task/Small/perfect_@{}.txt'.format(BEAM_SIZE),'a+') as fwrite:\n",
        "                fwrite.write('[+] input: {}\\n'.format(inputs[idx]))\n",
        "                fwrite.write('[*] target: {}\\n'.format(references[idx]))\n",
        "                fwrite.write('[-] pred:  {}\\n\\n'.format(pred))\n",
        "            \n",
        "            flag_perfect = True\n",
        "        \n",
        "        else:\n",
        "          mispred_list.append(pred)\n",
        "      \n",
        "    fpred.write('************************************\\n')\n",
        "        \n",
        "    idx += 1\n",
        "\n",
        "fpred.close()\n",
        "print('% of perfect predictions: ',(counter_pred/len(references))*100 )\n",
        "print(counter_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLOxaqzJdQtt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "generate_results.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}